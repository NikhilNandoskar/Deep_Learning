{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_10_DNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/NikhilNandoskar/Deep_Learning/blob/master/CIFAR_10_DNN.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "5aPdvkC_hALG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "2e98a2dd-ad4f-4496-faf5-160fc7a09028"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install tqdm\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "from os.path import isfile, isdir\n",
        "from tqdm import tqdm \n",
        "#import unittest \n",
        "from unittest import *\n",
        "import tarfile\n",
        "\n",
        "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
        "\n",
        "class DownloadProgress(tqdm):\n",
        "    last_block = 0\n",
        "\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
        "        self.total = total_size\n",
        "        self.update((block_num - self.last_block) * block_size)\n",
        "        self.last_block = block_num\n",
        "\n",
        "\"\"\" \n",
        "    check if the data (zip) file is already downloaded\n",
        "    if not, download it from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" and save as cifar-10-python.tar.gz\n",
        "\"\"\"\n",
        "if not isfile('cifar-10-python.tar.gz'):\n",
        "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
        "        urlretrieve(\n",
        "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
        "            'cifar-10-python.tar.gz',\n",
        "            pbar.hook)\n",
        "\n",
        "if not isdir(cifar10_dataset_folder_path):\n",
        "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "\n",
        "\n",
        "#test.test_folder_path(cifar10_dataset_folder_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tqdm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e6/19dfaff08fcbee7f3453e5b537e65a8364f1945f921a36d08be1e2ff3475/tqdm-4.24.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "Successfully installed tqdm-4.24.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "CIFAR-10 Dataset: 171MB [00:17, 9.94MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "P5cxIEByhHXb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zFTTGr2-kcuD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "  import pickle\n",
        "  with open(file, 'rb') as fo:\n",
        "    cif_dict = pickle.load(fo, encoding = 'bytes')\n",
        "  return cif_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lCs9P_omkvgW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dirs = ['batches.meta', 'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']\n",
        "all_data = [0,1,2,3,4,5,6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CSXtjPaRVK8N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "ad71f6fd-35ed-483f-f3db-395e6981fa19"
      },
      "cell_type": "code",
      "source": [
        "for i, direc in zip(all_data, dirs):\n",
        "  all_data[i] = unpickle(cifar10_dataset_folder_path+direc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-547691c75b37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar10_dataset_folder_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdirec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-d526802b9088>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcif_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcif_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cifar-10-batches-pybatches.meta'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "X6PsL-JhJGQY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(vec, value = 10):\n",
        "  out = np.zeros((len(vec), value))\n",
        "  out[range(len(vec), vec)] = 1\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rs9RgkBWIrrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CifarHelper():\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.i = 0\n",
        "        \n",
        "        self.all_train_batches = [data_batch1,data_batch2,data_batch3,data_batch4,data_batch5]\n",
        "        self.test_batch = [test_batch]\n",
        "        \n",
        "        self.training_images = None\n",
        "        self.training_labels = None\n",
        "        \n",
        "        self.test_images = None\n",
        "        self.test_labels = None\n",
        "    \n",
        "    def set_up_images(self):\n",
        "        \n",
        "        print(\"Setting Up Training Images and Labels\")\n",
        "        \n",
        "        self.training_images = np.vstack([d[batch['data']] for d in self.all_train_batches])\n",
        "        train_len = len(self.training_images)\n",
        "        \n",
        "        self.training_images = self.training_images.reshape(train_len,3,32,32).transpose(0,2,3,1)/255\n",
        "        self.training_labels = one_hot_encode(np.hstack([d[batch['labels']] for d in self.all_train_batches]), 10)\n",
        "        \n",
        "        print(\"Setting Up Test Images and Labels\")\n",
        "        \n",
        "        self.test_images = np.vstack([d[batch['data']] for d in self.test_batch])\n",
        "        test_len = len(self.test_images)\n",
        "        \n",
        "        self.test_images = self.test_images.reshape(test_len,3,32,32).transpose(0,2,3,1)/255\n",
        "        self.test_labels = one_hot_encode(np.hstack([d[batch['labels']] for d in self.test_batch]), 10)\n",
        "\n",
        "        \n",
        "    def next_batch(self, batch_size):\n",
        "        x = self.training_images[self.i:self.i+batch_size].reshape(100,32,32,3)\n",
        "        y = self.training_labels[self.i:self.i+batch_size]\n",
        "        self.i = (self.i + batch_size) % len(self.training_images)\n",
        "        return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nkk3zB0rJtbi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "ff607c1f-090b-4893-929a-756287224085"
      },
      "cell_type": "code",
      "source": [
        "# Before Your tf.Session run these two lines\n",
        "ch = CifarHelper()\n",
        "ch.set_up_images()\n",
        "\n",
        "# During your session to grab the next batch use this line\n",
        "# (Just like we did for mnist.train.next_batch)\n",
        "# batch = ch.next_batch(100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Up Training Images and Labels\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-7c8f5c58dc04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCifarHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_up_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# During your session to grab the next batch use this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# (Just like we did for mnist.train.next_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-148-9a605537d76a>\u001b[0m in \u001b[0;36mset_up_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setting Up Training Images and Labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_train_batches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-148-9a605537d76a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setting Up Training Images and Labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_train_batches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3tS7mq2Mp-GE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TensoeFlow \n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ysNZh3pArcyt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, shape = [None, 32, 32, 3], name = 'input_x')\n",
        "y_truth = tf.placeholder(tf.float32, shape = [None, 10], name = 'output_y')\n",
        "hold_prob = tf.placeholder(tf.float32, name = 'hold_prob')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "98weLgyFxAKt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def max_pool(x):\n",
        "  return tf.nn.max_pool(x, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mzwUu3EDz_yZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating Layers\n",
        "\n",
        "def conv_net(x, hold_prob):\n",
        "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.1))\n",
        "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.1))\n",
        "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.1))\n",
        "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.1))\n",
        "\n",
        "\n",
        "    \n",
        "    conv_1 = tf.nn.conv2d(x, conv1_filter, strides = [1,1,1,1], padding = 'SAME')\n",
        "    conv_1 = tf.nn.relu(conv_1)\n",
        "    conv_1_pooling = max_pool(conv_1)\n",
        "    conv_1_bn = tf.layers.batch_normalization(conv_1_pooling)\n",
        "\n",
        "    conv_2 = tf.nn.conv2d(conv_1_bn, conv2_filter, strides = [1,1,1,1], padding = 'SAME')\n",
        "    conv_2 = tf.nn.relu(conv_2)\n",
        "    conv_2_pooling = max_pool(conv_2)\n",
        "    conv_2_bn = tf.layers.batch_normalization(conv_2_pooling)\n",
        "\n",
        "    conv_3 = tf.nn.conv2d(conv_2_bn, conv3_filter, strides = [1,1,1,1], padding = 'SAME')\n",
        "    conv_3 = tf.nn.relu(conv_3) \n",
        "    conv_3_pooling = max_pool(conv_3)\n",
        "    conv_3_bn = tf.layers.batch_normalization(conv_3_pooling)\n",
        "\n",
        "    conv_4 = tf.nn.conv2d(conv_3_bn, conv4_filter, strides = [1,1,1,1], padding = 'SAME')\n",
        "    conv_4 = tf.nn.relu(conv_4)\n",
        "    conv_4_pooling = max_pool(conv_4)\n",
        "    conv_4_bn = tf.layers.batch_normalization(conv_4_pooling)\n",
        "\n",
        "    flat = tf.contrib.layers.flatten(conv_4_bn)\n",
        "\n",
        "    full_lay_1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
        "    full_lay_1 = tf.nn.dropout(full_lay_1, keep_prob = hold_prob)\n",
        "    full_lay_1 = tf.layers.batch_normalization(full_lay_1)\n",
        "\n",
        "\n",
        "    full_lay_2 = tf.contrib.layers.fully_connected(inputs=full_lay_1, num_outputs=256, activation_fn=tf.nn.relu)\n",
        "    full_lay_2 = tf.nn.dropout(full_lay_2, keep_prob = hold_prob)\n",
        "    full_lay_2 = tf.layers.batch_normalization(full_lay_2)\n",
        "\n",
        "\n",
        "    full_lay_3 = tf.contrib.layers.fully_connected(inputs=full_lay_2, num_outputs=512, activation_fn=tf.nn.relu)\n",
        "    full_lay_3 = tf.nn.dropout(full_lay_3, keep_prob = hold_prob)\n",
        "    full_lay_3 = tf.layers.batch_normalization(full_lay_3)\n",
        "\n",
        "\n",
        "    full_lay_4 = tf.contrib.layers.fully_connected(inputs=full_lay_3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
        "    full_lay_4 = tf.nn.dropout(full_lay_4, keep_prob = hold_prob)\n",
        "    full_lay_4 = tf.layers.batch_normalization(full_lay_4)\n",
        "\n",
        "    out = tf.contrib.layers.fully_connected(inputs=full_lay_4, num_outputs=10, activation_fn=None)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hn_Ij5st6g_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = conv_net(x, hold_prob)\n",
        "#model = tf.identity(logits, name = 'logits')\n",
        "\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_pred, labels = y_truth ))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lvi08wxkBY57",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ray9wuTkDS--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "f3218435-83a7-412c-f555-024a90732106"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for i in range(5000):\n",
        "        batch = ch.next_batch(100)\n",
        "        sess.run(train, feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.7})\n",
        "        \n",
        "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
        "        if i%100 == 0:\n",
        "            \n",
        "            print('Currently on step {}'.format(i))\n",
        "            print('Accuracy is:')\n",
        "            # Test the Train Model\n",
        "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_truth,1))\n",
        "\n",
        "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
        "\n",
        "            print(sess.run(acc,feed_dict={x:ch.test_images,y_true:ch.test_labels,hold_prob:1.0}))\n",
        "            print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-ac66b97e30b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ch' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Iu2-_xqmJoPK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}